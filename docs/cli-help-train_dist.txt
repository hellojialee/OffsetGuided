 python3 train_dist.py --help
usage: train_dist.py [-h] [--logging-output LOGGING_OUTPUT]
                     [--logging-stdout LOGGING_STDOUT]
                     [--logging-write LOGGING_WRITE] [--debug] [-q]
                     [--shut-data-logging]
                     [--train-annotations TRAIN_ANNOTATIONS]
                     [--train-image-dir TRAIN_IMAGE_DIR]
                     [--val-annotations VAL_ANNOTATIONS]
                     [--val-image-dir VAL_IMAGE_DIR]
                     [--n-images-train N_IMAGES_TRAIN]
                     [--n-images-val N_IMAGES_VAL]
                     [--loader-workers LOADER_WORKERS]
                     [--batch-size BATCH_SIZE] [--train-shuffle]
                     [--val-shuffle] [--square-length SQUARE_LENGTH]
                     [--flip-prob FLIP_PROB] [--max-rotate MAX_ROTATE]
                     [--min-scale MIN_SCALE] [--max-scale MAX_SCALE]
                     [--max-translate MAX_TRANSLATE] [--debug-affine-show]
                     [--initialize-whole INITIALIZE_WHOLE]
                     [--checkpoint-whole CHECKPOINT_WHOLE] [--basenet BASENET]
                     [--two-scale] [--multi-scale] [--no-pretrain]
                     [--basenet-checkpoint BASENET_CHECKPOINT]
                     [--headnets HEADNETS [HEADNETS ...]]
                     [--strides STRIDES [STRIDES ...]] [--max-stride {64,128}]
                     [--include-spread] [--include-background]
                     [--include-scale] [--lambdas LAMBDAS [LAMBDAS ...]]
                     [--stack-weights STACK_WEIGHTS [STACK_WEIGHTS ...]]
                     [--hmp-loss {l2_loss,focal_l2_loss}]
                     [--offset-loss {offset_l1_loss,offset_laplace_loss}]
                     [--sqrt-re] [--scale-loss {scale_l1_loss}] [--ftao FTAO]
                     [--fgamma FGAMMA] [--lmargin LMARGIN]
                     [--gaussian-clip-thre GAUSSIAN_CLIP_THRE] [--sigma SIGMA]
                     [--fill-scale-size FILL_SCALE_SIZE]
                     [--min_scale MIN_SCALE] [--resume] [--drop-optim-state]
                     [--freeze] [--drop-layers] [--epochs N] [--recount-epoch]
                     [--warmup] [--checkpoint-path CHECKPOINT_PATH]
                     [--max-grad-norm MAX_GRAD_NORM] [--local_rank LOCAL_RANK]
                     [--opt-level OPT_LEVEL] [--no-sync-bn]
                     [--keep-batchnorm-fp32 KEEP_BATCHNORM_FP32]
                     [--loss-scale LOSS_SCALE] [--channels-last]
                     [--print-freq N] [--optimizer {sgd,adam}]
                     [--learning-rate LR] [--momentum M] [--weight-decay W]

Distributed training with Nvidia Apex

optional arguments:
  -h, --help            show this help message and exit
  --resume, -r          resume from checkpoint (default: False)
  --drop-optim-state    do not resume the optimizer from checkpoint. (default:
                        True)
  --freeze              freeze the pre-trained layers of the BaseNet, i.e.
                        Backbone (default: False)
  --drop-layers         drop some layers described in
                        models.networks.load_model (default: False)
  --epochs N            number of epochs to train (default: 100)
  --recount-epoch       reset the epoch counter to 0 (default: False)
  --warmup              using warm-up learning rate (default: False)
  --checkpoint-path CHECKPOINT_PATH, -p CHECKPOINT_PATH
                        folder path checkpoint storage of the whole pose
                        estimation model (default: link2checkpoints_storage)
  --max-grad-norm MAX_GRAD_NORM
                        If the norm of the gradient vector exceeds this, re-
                        normalize it to have the norm equal to max_grad_norm
                        (default: inf)

logging:
  --logging-output LOGGING_OUTPUT
                        path to write the log (default: None)
  --logging-stdout LOGGING_STDOUT
                        print the detailed log at stdout stream (default:
                        False)
  --logging-write LOGGING_WRITE
                        write the detailed log into log file (default: True)
  --debug               print debug messages (default: False)
  -q, --quiet           only show warning messages or above (default: False)
  --shut-data-logging   shut up the logging info during data preparing
                        (default: False)

dataset and loader:
  --train-annotations TRAIN_ANNOTATIONS
  --train-image-dir TRAIN_IMAGE_DIR
  --val-annotations VAL_ANNOTATIONS
  --val-image-dir VAL_IMAGE_DIR
  --n-images-train N_IMAGES_TRAIN
                        number of images to sample from the trains subset
                        (default: None)
  --n-images-val N_IMAGES_VAL
                        number of images to sample from the val subset
                        (default: None)
  --loader-workers LOADER_WORKERS
                        number of workers for data loading (default: 8)
  --batch-size BATCH_SIZE
                        batch size (default: 8)
  --train-shuffle       force the trains dataset shuffle by hand (default:
                        False)
  --val-shuffle         force the validate dataset shuffle by hand (default:
                        False)

training parameters for warp affine:
  --square-length SQUARE_LENGTH
                        square edge of input images (default: 512)
  --flip-prob FLIP_PROB
                        the probability to flip the input image (default: 0.5)
  --max-rotate MAX_ROTATE
  --min-scale MIN_SCALE
                        lower bound of the relative image scale during
                        augmentation (default: 0.7)
  --max-scale MAX_SCALE
  --max-translate MAX_TRANSLATE
                        upper bound of shitting the image during augmentation
                        (default: 50)
  --debug-affine-show   show the transformed image and keyooints (default:
                        False)

model configuration:
  --initialize-whole INITIALIZE_WHOLE
                        randomly initialize the basenet and headnets, just set
                        it to True if you are not certain (default: True)
  --checkpoint-whole CHECKPOINT_WHOLE
                        the checkpoint pach to the whole model
                        (basenet+headnets) (default: None)

base network configuration:
  --basenet BASENET     base network, e.g. hourglass4stage (default:
                        hourglass104)
  --two-scale           to be implemented (default: False)
  --multi-scale         to be implemented (default: False)
  --no-pretrain         create BaseNet without pretraining (default: True)
  --basenet-checkpoint BASENET_CHECKPOINT
                        Path to the pre-trained model and optimizer. (default:
                        weights/hourglass_104_renamed.pth)

head network configuration:
  --headnets HEADNETS [HEADNETS ...]
                        head networks (default: ['hmp', 'omp'])
  --strides STRIDES [STRIDES ...]
                        rations of the input to the output of basenet, also
                        the strides of all sub headnets. Also, they determin
                        the strides in encoder and decoder (default: [4, 4])
  --max-stride {64,128}
                        the max down-sampling stride through the network.
                        (default: 128)
  --include-spread      add conv layers to regress the spread_b of Laplace
                        distribution, you should set it to True if you chose
                        laplace loss (default: False)
  --include-background  include the heatmap of background channel (default:
                        False)
  --include-scale       add cone layers to regress the keypoint scales in
                        separate channels (default: False)

loss configuration:
  --lambdas LAMBDAS [LAMBDAS ...]
                        learning task scaling factors for hmp_loss,
                        bg_hmp_loss, offset_loss and scale_loss, directly
                        multiplied, not averaged (default: [1, 1, 0.01, 1])
  --stack-weights STACK_WEIGHTS [STACK_WEIGHTS ...]
                        loss weights for different stacks, weighted-sum
                        averaged (default: [1, 1])
  --hmp-loss {l2_loss,focal_l2_loss}
                        loss for heatmap regression (default: focal_l2_loss)
  --offset-loss {offset_l1_loss,offset_laplace_loss}
                        loss for offeset regression (default: offset_l1_loss)
  --sqrt-re             rescale the offset loss using torch.sqrt (default:
                        False)
  --scale-loss {scale_l1_loss}
                        loss for keypoint scale regression (default:
                        scale_l1_loss)
  --ftao FTAO           threshold between fore/background in focal L2 loss
                        during training (default: 0.01)
  --fgamma FGAMMA       order of scaling factor in focal L2 loss during
                        training (default: 1)
  --lmargin LMARGIN     offset length below this value will not be punished
                        during training (default: 0.1)

heatmap encoder:
  --gaussian-clip-thre GAUSSIAN_CLIP_THRE
                        Gaussian distribution below this value is cut to zero
                        (default: 0.01)
  --sigma SIGMA         standard deviation of Gaussian distribution (default:
                        9)

offsetmap and scalemap encoder:
  --fill-scale-size FILL_SCALE_SIZE
                        the area around the keypoint will be filled with joint
                        scale values. (default: 10)
  --min_scale MIN_SCALE
                        set minimum keypoint scale (default: 1)

apex configuration:
  --local_rank LOCAL_RANK
  --opt-level OPT_LEVEL
  --no-sync-bn          enabling apex sync BN. (default: True)
  --keep-batchnorm-fp32 KEEP_BATCHNORM_FP32
  --loss-scale LOSS_SCALE
  --channels-last       channel last may lead to 22{'option_strings': ['--
                        channels-last'], 'dest': 'channels_last', 'nargs': 0,
                        'const': True, 'default': False, 'type': None,
                        'choices': None, 'required': False, 'help': 'channel
                        last may lead to 22% speed up', 'metavar': None,
                        'container': <argparse._ArgumentGroup object at
                        0x7fb5bc964240>, 'prog': 'train_dist.py'}peed up
                        (default: False)
  --print-freq N, -f N  print frequency (default: 10) (default: 10)

optimizer configuration:
  --optimizer {sgd,adam}
  --learning-rate LR    learning rate (default: 0.00025)
  --momentum M          momentum for sgd (default: 0.9)
  --weight-decay W, --wd W
                        weight decay (e.g. 1e-4) (default: 0)
